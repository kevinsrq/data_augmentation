{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "import config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {'Historia': 0, 'Administracao': 1, 'Geografia': 2, 'Biologia': 3, \n",
    "          'Literatura': 4, 'Artes': 5, 'Matematica': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('../data/processed/train_final.csv')\n",
    "x_test = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "x_test.drop(['titulo', 'genero'], axis=1, inplace=True)\n",
    "\n",
    "# x_train['alt_title'] = x_train['alt_title'].map(lambda title: \\\n",
    "#                                                 tokenizer.encode_plus(title, \n",
    "#                                                                       add_special_tokens=True,\n",
    "#                                                                       padding='longest'))\n",
    "# x_test['alt_title'] = x_test['alt_title'].map(lambda title: \\\n",
    "#                                               tokenizer.encode_plus(title, \n",
    "#                                                                     add_special_tokens=True,\n",
    "#                                                                     padding='longest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len: int = None) -> object:\n",
    "        \n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = max_len \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        textos = self.data.loc[index, 'alt_title'], \n",
    "        labels = self.data.loc[index, 'label']\n",
    "\n",
    "        encoding = tokenizer(textos, \n",
    "                             max_length=self.max_len,\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "        \n",
    "        inputs = encoding['input_ids']\n",
    "        tkn_type = encoding['token_type_ids']\n",
    "        att_mask = encoding['attention_mask']\n",
    "\n",
    "        return inputs, tkn_type, att_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train, tokenizer=tokenizer, max_len=32)\n",
    "test_dataset = MyDataset(x_test, tokenizer=tokenizer, max_len=32)\n",
    "\n",
    "del x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "class CustomBERTModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained BERT model and tokenizer\n",
    "        # self.bert = pt_bert_model \n",
    "        self.bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "        \n",
    "        for p in self.bert.embeddings.parameters(): \n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through the BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\t\n",
    "        pooled_output  = outputs.pooler_output\n",
    "\n",
    "        # Pass the pooled output through the classification layer\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        # Apply softmax activation function to the classification output\n",
    "        probabilities = self.softmax(logits)\n",
    "\n",
    "        return probabilities, logits\n",
    "\n",
    "model = CustomBERTModel(num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "\n",
    "        # Load the pre-trained BERT model and tokenizer\n",
    "        self.bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "        # Optionally, you can choose to freeze the embeddings\n",
    "        for p in self.bert.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)  # Additional linear layer\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(256, 128)  # Additional linear layer\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_tp_ids):\n",
    "        # Forward pass through the BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_tp_ids)\n",
    "\n",
    "        # Extract the last hidden state of the token [CLS] for classification task\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Apply the additional linear layer with ReLU activation\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Pass the transformed output through the classification layer\n",
    "        logits = self.fc5(x)\n",
    "        print(logits)\n",
    "        return logits  # Softmax is applied outside the model during training\n",
    "\n",
    "model = CustomBERTModel(num_classes=7)\n",
    "# model = CustomBERTModel(num_classes=7)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "# model = CustomBERTModel(num_classes=7, pt_bert_model=emb_model)\n",
    "model = CustomBERTModel(num_classes=7)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10): \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (inputs, tkn_type, att_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        probabilities, logits = model(inputs[0], att_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0623e+00, -6.3325e-01, -1.2283e+00,  7.9033e-01, -6.5021e-01,\n",
      "         -7.9138e-01, -3.5793e-01],\n",
      "        [-8.6612e-01, -5.9101e-01, -1.1515e+00,  9.1877e-01, -6.2645e-01,\n",
      "         -5.5637e-01, -1.6336e-01],\n",
      "        [-2.6077e+00, -1.9602e+00, -1.6902e+00,  1.8753e+00, -6.8836e-02,\n",
      "         -1.2428e+00, -9.9353e-01],\n",
      "        [-1.3198e+00, -1.0981e+00, -1.5834e+00,  1.2386e+00, -6.2984e-01,\n",
      "         -8.7544e-01, -4.8791e-01],\n",
      "        [-7.8601e-01, -4.6145e-01, -1.0827e+00,  8.7943e-01, -6.0326e-01,\n",
      "         -5.1806e-01, -2.4364e-01],\n",
      "        [-6.2665e-01, -2.7670e-01, -8.5965e-01,  6.6843e-01, -5.9180e-01,\n",
      "         -3.9709e-01, -2.1887e-01],\n",
      "        [-1.5117e+00, -1.3779e+00, -1.8075e+00,  1.4690e+00, -6.5238e-01,\n",
      "         -1.0756e+00, -7.7838e-01],\n",
      "        [-6.5022e-01, -1.4172e-01, -7.7179e-01,  5.2761e-01, -6.0880e-01,\n",
      "         -4.1640e-01, -6.8984e-02],\n",
      "        [-1.8651e+00, -1.4301e+00, -1.6874e+00,  1.4307e+00, -4.5705e-01,\n",
      "         -1.0413e+00, -7.6289e-01],\n",
      "        [-7.4264e-01, -2.3616e-01, -7.9036e-01,  5.3140e-01, -5.7464e-01,\n",
      "         -4.5433e-01, -8.4253e-02],\n",
      "        [-7.2948e-01, -2.1545e-01, -8.7586e-01,  5.7140e-01, -6.3687e-01,\n",
      "         -4.4391e-01, -1.6684e-01],\n",
      "        [-2.3406e+00, -1.6820e+00, -2.1684e+00,  1.7368e+00, -6.7176e-01,\n",
      "         -1.2460e+00, -7.5651e-01],\n",
      "        [-6.3472e-01, -1.2834e-01, -6.1971e-01,  3.7886e-01, -5.3126e-01,\n",
      "         -4.3186e-01, -1.0659e-01],\n",
      "        [-8.8476e-01, -3.9844e-01, -5.8826e-01,  3.8916e-01, -3.6436e-01,\n",
      "         -4.9402e-01, -2.1359e-01],\n",
      "        [-6.3732e-01, -6.1855e-01, -1.0476e+00,  8.6146e-01, -5.7925e-01,\n",
      "         -6.4123e-01, -4.3626e-01],\n",
      "        [-1.5898e+00, -8.1295e-01, -9.8104e-01,  7.1250e-01, -2.6720e-01,\n",
      "         -7.3795e-01, -4.2931e-01],\n",
      "        [-4.7802e-01, -1.5565e-01, -5.7789e-01,  3.6672e-01, -5.3894e-01,\n",
      "         -5.0212e-01, -1.0887e-01],\n",
      "        [-7.2242e-01, -3.6876e-01, -7.1719e-01,  4.6020e-01, -4.9238e-01,\n",
      "         -5.7624e-01, -2.3755e-01],\n",
      "        [-1.2471e+00, -1.1046e+00, -1.3287e+00,  1.1879e+00, -4.1396e-01,\n",
      "         -9.0142e-01, -7.6127e-01],\n",
      "        [-1.3711e+00, -1.2845e+00, -1.4766e+00,  1.2833e+00, -5.0137e-01,\n",
      "         -9.6082e-01, -8.2918e-01],\n",
      "        [-8.6703e-01, -7.5624e-01, -1.2508e+00,  9.7570e-01, -6.4978e-01,\n",
      "         -7.0758e-01, -3.4056e-01],\n",
      "        [-1.0381e+00, -3.8286e-01, -1.2017e+00,  7.7712e-01, -7.4103e-01,\n",
      "         -5.4764e-01, -1.0832e-01],\n",
      "        [-5.2731e-01, -1.2943e-01, -5.6765e-01,  4.0743e-01, -4.6148e-01,\n",
      "         -4.1982e-01, -1.6245e-01],\n",
      "        [-8.1273e-01, -4.1458e-01, -1.1605e+00,  6.8735e-01, -8.1678e-01,\n",
      "         -6.4448e-01, -1.2255e-01],\n",
      "        [-1.7749e+00, -1.4533e+00, -1.7635e+00,  1.5262e+00, -5.1928e-01,\n",
      "         -1.1781e+00, -8.0444e-01],\n",
      "        [-1.2246e+00, -4.0855e-01, -1.1219e+00,  6.9335e-01, -6.4507e-01,\n",
      "         -5.5020e-01,  6.9782e-05],\n",
      "        [-1.3470e+00, -1.2613e+00, -1.5163e+00,  1.3348e+00, -4.9682e-01,\n",
      "         -8.2593e-01, -7.2099e-01],\n",
      "        [-1.4844e+00, -9.4529e-01, -1.3467e+00,  9.6224e-01, -4.6321e-01,\n",
      "         -9.2490e-01, -4.8740e-01],\n",
      "        [-1.1641e+00, -3.5381e-01, -1.2016e+00,  6.9613e-01, -7.8460e-01,\n",
      "         -5.8488e-01, -1.8307e-02],\n",
      "        [-1.3776e+00, -1.1359e+00, -1.3432e+00,  1.1007e+00, -4.3569e-01,\n",
      "         -9.4332e-01, -6.6925e-01],\n",
      "        [-4.8027e-01, -1.4180e-01, -7.7634e-01,  6.3076e-01, -5.0494e-01,\n",
      "         -3.1497e-01, -8.3587e-02],\n",
      "        [-9.4589e-01, -5.4551e-01, -1.0908e+00,  7.7743e-01, -5.4991e-01,\n",
      "         -6.6356e-01, -3.6023e-01],\n",
      "        [-1.5728e+00, -1.1686e+00, -1.9871e+00,  1.5660e+00, -8.8547e-01,\n",
      "         -9.5749e-01, -4.6954e-01],\n",
      "        [-2.9096e-01,  4.0564e-02, -3.0634e-01,  2.0652e-01, -4.2928e-01,\n",
      "         -3.1561e-01, -1.0219e-01],\n",
      "        [-4.4016e-01, -3.9278e-02, -3.1250e-01,  1.4335e-01, -4.2314e-01,\n",
      "         -3.8908e-01, -7.0661e-02],\n",
      "        [-8.3881e-01, -2.1111e-01, -1.0683e+00,  6.1090e-01, -7.9985e-01,\n",
      "         -5.6645e-01, -1.3969e-01],\n",
      "        [-1.4148e+00, -6.4452e-01, -1.4263e+00,  1.1363e+00, -6.6112e-01,\n",
      "         -5.9829e-01, -1.9597e-01],\n",
      "        [-2.0233e+00, -1.3200e+00, -1.2115e+00,  1.2208e+00, -1.3040e-01,\n",
      "         -8.9644e-01, -6.7682e-01],\n",
      "        [-5.7011e-01, -1.5641e-01, -6.4910e-01,  4.0360e-01, -5.4873e-01,\n",
      "         -4.5138e-01, -9.5219e-02],\n",
      "        [-1.9221e+00, -7.9159e-01, -8.9364e-01,  6.5566e-01, -1.2869e-01,\n",
      "         -7.3170e-01, -3.4847e-01],\n",
      "        [-9.1828e-01, -7.2880e-01, -1.1650e+00,  8.9464e-01, -5.5822e-01,\n",
      "         -7.4530e-01, -4.3638e-01],\n",
      "        [-6.7732e-01,  8.3584e-02, -5.4103e-01,  2.2064e-01, -5.9155e-01,\n",
      "         -4.0746e-01,  3.3318e-02],\n",
      "        [-1.0854e+00, -3.2772e-01, -8.8132e-01,  4.1452e-01, -5.9499e-01,\n",
      "         -5.9573e-01, -3.9213e-02],\n",
      "        [-8.2067e-01, -5.0003e-01, -8.4573e-01,  5.6150e-01, -4.9959e-01,\n",
      "         -6.2303e-01, -2.8867e-01],\n",
      "        [-7.6480e-01, -3.0629e-01, -8.0297e-01,  4.9978e-01, -5.4711e-01,\n",
      "         -5.7213e-01, -1.9614e-01],\n",
      "        [-7.4173e-01, -2.8867e-01, -8.9239e-01,  6.7637e-01, -5.7340e-01,\n",
      "         -4.1761e-01, -1.4697e-01],\n",
      "        [-7.5592e-01, -4.3346e-01, -7.8015e-01,  5.3079e-01, -4.7607e-01,\n",
      "         -6.3542e-01, -2.8645e-01],\n",
      "        [-1.8636e+00, -1.4178e+00, -1.7614e+00,  1.3911e+00, -5.9062e-01,\n",
      "         -1.0507e+00, -7.1336e-01],\n",
      "        [-1.6923e+00, -1.3594e+00, -2.1174e+00,  1.6068e+00, -8.3668e-01,\n",
      "         -1.1869e+00, -6.9533e-01],\n",
      "        [-1.5236e+00, -1.2443e+00, -1.5521e+00,  1.3034e+00, -4.8847e-01,\n",
      "         -1.0030e+00, -7.5081e-01],\n",
      "        [-6.4182e-01, -1.9240e-01, -7.8014e-01,  5.7106e-01, -6.0842e-01,\n",
      "         -4.3193e-01, -1.4246e-01],\n",
      "        [-1.0472e+00, -2.7916e-01, -4.8355e-01,  2.9485e-01, -2.8243e-01,\n",
      "         -5.1439e-01, -6.0914e-02],\n",
      "        [-1.0116e+00, -2.1539e-01, -6.8877e-01,  2.8369e-01, -4.7082e-01,\n",
      "         -5.8919e-01, -6.1067e-02],\n",
      "        [-1.4813e+00, -1.0092e+00, -1.2790e+00,  9.7851e-01, -3.9717e-01,\n",
      "         -8.9438e-01, -5.4969e-01],\n",
      "        [-7.7237e-01, -3.4505e-01, -1.0970e+00,  7.2855e-01, -7.5618e-01,\n",
      "         -4.9021e-01, -1.9945e-01],\n",
      "        [-1.3750e+00, -1.2601e+00, -1.9312e+00,  1.5329e+00, -8.4889e-01,\n",
      "         -9.8085e-01, -5.9917e-01],\n",
      "        [-2.7826e+00, -2.2719e+00, -2.7237e+00,  2.3859e+00, -7.7500e-01,\n",
      "         -1.6050e+00, -1.0048e+00],\n",
      "        [-9.2042e-01, -6.3221e-01, -1.3513e+00,  9.4455e-01, -7.6547e-01,\n",
      "         -6.8813e-01, -3.6446e-01],\n",
      "        [-1.2613e+00, -1.1309e+00, -1.6598e+00,  1.3290e+00, -6.9881e-01,\n",
      "         -9.1605e-01, -5.7095e-01],\n",
      "        [-2.8732e-01, -1.2926e-02, -4.5185e-01,  3.6319e-01, -4.3489e-01,\n",
      "         -3.1277e-01, -1.7842e-01],\n",
      "        [-1.5814e+00, -1.0503e+00, -1.6459e+00,  1.1507e+00, -6.5127e-01,\n",
      "         -9.9204e-01, -4.5751e-01],\n",
      "        [-6.3393e-01, -1.2029e-01, -7.4533e-01,  3.7772e-01, -6.7649e-01,\n",
      "         -4.9613e-01, -3.2799e-02],\n",
      "        [-1.3652e+00, -1.2911e+00, -1.9751e+00,  1.4648e+00, -8.8458e-01,\n",
      "         -1.0482e+00, -6.0104e-01],\n",
      "        [-1.1742e+00, -8.2491e-01, -1.2429e+00,  8.5814e-01, -5.6260e-01,\n",
      "         -8.4971e-01, -4.0737e-01],\n",
      "        [-1.4230e+00, -1.3219e+00, -1.5743e+00,  1.3624e+00, -4.9950e-01,\n",
      "         -9.6538e-01, -7.9262e-01],\n",
      "        [-6.9353e-01, -4.3859e-01, -7.9062e-01,  5.6688e-01, -4.7730e-01,\n",
      "         -6.1964e-01, -2.9112e-01],\n",
      "        [-7.0872e-01, -8.7112e-01, -1.1621e+00,  1.0019e+00, -5.3458e-01,\n",
      "         -7.8236e-01, -6.8233e-01],\n",
      "        [-1.3299e+00, -1.0716e+00, -1.1637e+00,  1.0025e+00, -3.7003e-01,\n",
      "         -7.8861e-01, -6.6088e-01],\n",
      "        [-1.4454e+00, -7.9558e-01, -1.3287e+00,  8.1393e-01, -6.1208e-01,\n",
      "         -8.1171e-01, -3.7547e-01],\n",
      "        [-3.0661e-01,  1.8775e-02, -3.6039e-01,  2.0314e-01, -4.2339e-01,\n",
      "         -3.5009e-01, -1.1517e-01],\n",
      "        [-9.6731e-01, -3.1232e-01, -6.4281e-01,  3.6099e-01, -4.2081e-01,\n",
      "         -5.4218e-01, -1.4189e-01],\n",
      "        [-8.2163e-01, -2.8700e-01, -6.6362e-01,  3.7822e-01, -4.7630e-01,\n",
      "         -4.2417e-01, -1.1111e-01],\n",
      "        [-4.7787e-01,  1.5400e-01, -5.5279e-01,  4.1681e-01, -4.0479e-01,\n",
      "         -2.3945e-01, -3.3096e-02],\n",
      "        [-5.9772e-01, -1.4801e-01, -7.9985e-01,  6.3021e-01, -5.2683e-01,\n",
      "         -3.5375e-01, -1.7792e-01],\n",
      "        [-1.1732e+00, -1.3349e+00, -1.7860e+00,  1.4861e+00, -7.4146e-01,\n",
      "         -9.6466e-01, -7.9474e-01],\n",
      "        [-1.3720e+00, -1.1515e+00, -1.4233e+00,  1.1145e+00, -4.9880e-01,\n",
      "         -9.2721e-01, -5.9417e-01],\n",
      "        [-1.0230e+00, -4.7066e-01, -1.3848e+00,  1.0960e+00, -7.5808e-01,\n",
      "         -5.2763e-01, -1.5750e-01],\n",
      "        [-1.2944e+00, -1.1529e+00, -1.7918e+00,  1.4810e+00, -7.3269e-01,\n",
      "         -8.5034e-01, -6.8518e-01],\n",
      "        [-8.0170e-01, -4.3783e-01, -1.0156e+00,  7.2946e-01, -5.9520e-01,\n",
      "         -5.6765e-01, -2.6727e-01],\n",
      "        [-1.3152e+00, -6.3868e-01, -8.7588e-01,  5.8204e-01, -3.1005e-01,\n",
      "         -6.6457e-01, -2.8320e-01],\n",
      "        [-1.1400e+00, -7.6542e-01, -1.4954e+00,  1.0684e+00, -8.0616e-01,\n",
      "         -7.0967e-01, -2.9034e-01],\n",
      "        [-8.3090e-01, -4.3523e-01, -1.1579e+00,  7.7509e-01, -7.4155e-01,\n",
      "         -5.4804e-01, -2.3987e-01],\n",
      "        [-1.0283e+00, -3.3518e-01, -9.0061e-01,  4.7918e-01, -5.7044e-01,\n",
      "         -6.4165e-01, -1.0401e-01],\n",
      "        [-1.9713e+00, -1.6138e+00, -1.4627e+00,  1.4853e+00, -2.3973e-01,\n",
      "         -9.0483e-01, -9.7506e-01],\n",
      "        [-6.8772e-01, -2.8811e-01, -7.1322e-01,  4.3068e-01, -5.2981e-01,\n",
      "         -5.8443e-01, -2.2997e-01],\n",
      "        [-1.6845e+00, -1.2645e+00, -1.2890e+00,  1.1800e+00, -2.5670e-01,\n",
      "         -9.1015e-01, -7.8932e-01],\n",
      "        [-1.0137e+00, -9.3936e-01, -1.2545e+00,  1.0080e+00, -5.3752e-01,\n",
      "         -7.9240e-01, -5.3761e-01],\n",
      "        [-1.1794e+00, -6.1401e-01, -9.3136e-01,  6.3021e-01, -4.2279e-01,\n",
      "         -6.8020e-01, -2.7158e-01],\n",
      "        [-1.4997e+00, -1.2486e+00, -1.8347e+00,  1.3700e+00, -7.5575e-01,\n",
      "         -1.1204e+00, -6.1214e-01],\n",
      "        [-1.2683e+00, -1.0484e+00, -1.6151e+00,  1.2591e+00, -7.2228e-01,\n",
      "         -8.4148e-01, -4.4277e-01],\n",
      "        [-1.6847e+00, -1.3116e+00, -1.5576e+00,  1.2882e+00, -4.7801e-01,\n",
      "         -9.9071e-01, -7.1159e-01],\n",
      "        [-1.2067e+00, -9.9294e-01, -1.3352e+00,  1.0482e+00, -5.3110e-01,\n",
      "         -9.0231e-01, -5.6282e-01],\n",
      "        [-1.2882e+00, -1.1514e+00, -1.3355e+00,  1.1731e+00, -3.7287e-01,\n",
      "         -8.6294e-01, -6.6323e-01],\n",
      "        [-2.8172e-01,  1.5337e-01, -4.2269e-01,  3.1471e-01, -4.3898e-01,\n",
      "         -2.2915e-01, -1.5094e-01],\n",
      "        [-2.8658e-01, -2.1989e-01, -5.1787e-01,  4.3927e-01, -4.5317e-01,\n",
      "         -4.1338e-01, -2.1888e-01],\n",
      "        [-1.1124e+00, -6.5061e-01, -9.9085e-01,  6.6213e-01, -4.7722e-01,\n",
      "         -7.3093e-01, -3.7203e-01],\n",
      "        [-2.5344e-01, -3.1220e-02, -5.7684e-01,  5.1812e-01, -4.6077e-01,\n",
      "         -2.9665e-01, -2.6881e-01],\n",
      "        [-2.2464e-01,  2.7062e-01, -4.1003e-01,  3.2558e-01, -4.0250e-01,\n",
      "         -1.5934e-01, -1.1556e-01],\n",
      "        [-1.5631e+00, -1.4800e+00, -1.9654e+00,  1.5962e+00, -7.5273e-01,\n",
      "         -1.1254e+00, -8.2090e-01],\n",
      "        [-1.5951e+00, -1.4389e+00, -1.5347e+00,  1.4274e+00, -3.8578e-01,\n",
      "         -8.9628e-01, -8.4215e-01],\n",
      "        [-9.4603e-01, -2.1994e-01, -5.7115e-01,  2.4410e-01, -3.9831e-01,\n",
      "         -6.0475e-01, -4.6775e-02],\n",
      "        [-1.7638e+00, -1.7760e+00, -2.0545e+00,  1.9646e+00, -5.4352e-01,\n",
      "         -1.0886e+00, -1.0207e+00],\n",
      "        [-1.3078e+00, -1.5639e+00, -1.9171e+00,  1.7016e+00, -7.2570e-01,\n",
      "         -1.0594e+00, -9.5413e-01],\n",
      "        [-1.0088e+00, -5.3875e-01, -9.5474e-01,  5.8440e-01, -5.3234e-01,\n",
      "         -6.2124e-01, -2.7088e-01],\n",
      "        [-2.6331e-01,  1.1582e-01, -2.1170e-01,  1.3110e-01, -3.7789e-01,\n",
      "         -3.2875e-01, -8.9212e-02],\n",
      "        [-7.9912e-01, -3.1429e-01, -8.9793e-01,  6.9759e-01, -5.6959e-01,\n",
      "         -3.9383e-01, -9.3161e-02],\n",
      "        [-7.2146e-01, -2.3943e-01, -8.9781e-01,  6.7569e-01, -5.5273e-01,\n",
      "         -4.3805e-01, -2.4397e-01],\n",
      "        [-5.5528e-01, -7.7198e-02, -5.2177e-01,  3.6142e-01, -4.6048e-01,\n",
      "         -3.6948e-01, -1.0535e-01],\n",
      "        [-1.3403e+00, -9.5486e-01, -1.2838e+00,  9.3115e-01, -4.8487e-01,\n",
      "         -8.1018e-01, -4.3694e-01],\n",
      "        [-1.1483e+00, -1.1016e+00, -1.7720e+00,  1.3497e+00, -8.5255e-01,\n",
      "         -8.8028e-01, -5.5239e-01],\n",
      "        [-8.2895e-01, -3.6227e-01, -1.2040e+00,  8.3470e-01, -7.9471e-01,\n",
      "         -4.9905e-01, -1.9487e-01],\n",
      "        [-7.4488e-01, -5.0565e-01, -1.0226e+00,  7.6789e-01, -6.0645e-01,\n",
      "         -5.4202e-01, -3.4208e-01],\n",
      "        [-1.4560e+00, -6.4196e-01, -1.7207e+00,  1.1090e+00, -9.6517e-01,\n",
      "         -7.2558e-01, -1.3640e-01],\n",
      "        [-2.1842e+00, -1.7417e+00, -1.7066e+00,  1.6602e+00, -2.8717e-01,\n",
      "         -1.1247e+00, -1.0362e+00],\n",
      "        [-6.4628e-01, -1.1009e-01, -6.9824e-01,  3.2832e-01, -6.3890e-01,\n",
      "         -5.4660e-01, -7.1494e-02],\n",
      "        [-2.3893e+00, -2.2261e+00, -1.7863e+00,  2.1443e+00, -1.1205e-01,\n",
      "         -1.2173e+00, -1.3144e+00],\n",
      "        [-1.1331e+00, -6.3963e-01, -9.4569e-01,  6.5520e-01, -4.0240e-01,\n",
      "         -7.1201e-01, -3.4220e-01],\n",
      "        [-1.1511e+00, -1.1574e+00, -1.5154e+00,  1.3058e+00, -5.5473e-01,\n",
      "         -9.7383e-01, -7.2614e-01],\n",
      "        [-8.2636e-01, -7.9565e-01, -9.9951e-01,  8.4289e-01, -4.4106e-01,\n",
      "         -7.4575e-01, -5.8457e-01],\n",
      "        [-1.4442e+00, -7.3032e-01, -7.5220e-01,  5.9740e-01, -1.4389e-01,\n",
      "         -6.3665e-01, -4.0492e-01],\n",
      "        [-5.2014e-01, -1.8325e-01, -5.1344e-01,  3.2051e-01, -4.7457e-01,\n",
      "         -4.0098e-01, -1.6371e-01],\n",
      "        [-9.2691e-01, -9.6524e-01, -1.3855e+00,  1.2563e+00, -5.8670e-01,\n",
      "         -6.8720e-01, -5.4829e-01],\n",
      "        [-9.7292e-01, -9.6196e-03, -5.1037e-01,  1.1374e-01, -4.7415e-01,\n",
      "         -5.0001e-01,  1.4791e-01],\n",
      "        [-6.2244e-01, -3.9136e-01, -9.8969e-01,  7.5046e-01, -6.3611e-01,\n",
      "         -4.8851e-01, -3.2188e-01],\n",
      "        [-8.9812e-01, -6.3508e-01, -1.0832e+00,  8.8249e-01, -5.2556e-01,\n",
      "         -5.7206e-01, -2.6775e-01],\n",
      "        [-8.7385e-01, -7.3676e-01, -1.2399e+00,  9.3207e-01, -6.4421e-01,\n",
      "         -6.7557e-01, -3.8012e-01],\n",
      "        [-8.0121e-01, -3.4555e-01, -1.1078e+00,  7.1533e-01, -7.8150e-01,\n",
      "         -6.1983e-01, -1.7380e-01],\n",
      "        [-1.0492e+00, -8.1279e-01, -1.6632e+00,  1.1363e+00, -9.6048e-01,\n",
      "         -8.2960e-01, -3.1559e-01]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.0685, 0.1052, 0.0580, 0.4367, 0.1034, 0.0898, 0.1385],\n",
      "        [0.0731, 0.0962, 0.0550, 0.4356, 0.0929, 0.0996, 0.1476],\n",
      "        [0.0087, 0.0165, 0.0217, 0.7661, 0.1096, 0.0339, 0.0435],\n",
      "        [0.0459, 0.0573, 0.0353, 0.5929, 0.0915, 0.0716, 0.1055],\n",
      "        [0.0791, 0.1094, 0.0588, 0.4183, 0.0950, 0.1034, 0.1361],\n",
      "        [0.0938, 0.1331, 0.0743, 0.3425, 0.0971, 0.1180, 0.1410],\n",
      "        [0.0350, 0.0400, 0.0260, 0.6894, 0.0826, 0.0541, 0.0729],\n",
      "        [0.0918, 0.1527, 0.0813, 0.2982, 0.0957, 0.1160, 0.1642],\n",
      "        [0.0249, 0.0385, 0.0298, 0.6730, 0.1019, 0.0568, 0.0751],\n",
      "        [0.0859, 0.1426, 0.0819, 0.3072, 0.1017, 0.1147, 0.1660],\n",
      "        [0.0878, 0.1468, 0.0758, 0.3224, 0.0963, 0.1168, 0.1541],\n",
      "        [0.0131, 0.0253, 0.0156, 0.7733, 0.0696, 0.0392, 0.0639],\n",
      "        [0.0956, 0.1586, 0.0971, 0.2634, 0.1060, 0.1171, 0.1621],\n",
      "        [0.0790, 0.1284, 0.1062, 0.2823, 0.1329, 0.1167, 0.1545],\n",
      "        [0.0958, 0.0976, 0.0636, 0.4289, 0.1015, 0.0954, 0.1171],\n",
      "        [0.0412, 0.0895, 0.0756, 0.4114, 0.1545, 0.0965, 0.1313],\n",
      "        [0.1114, 0.1538, 0.1008, 0.2593, 0.1048, 0.1088, 0.1611],\n",
      "        [0.0932, 0.1327, 0.0937, 0.3040, 0.1173, 0.1078, 0.1513],\n",
      "        [0.0504, 0.0582, 0.0465, 0.5757, 0.1160, 0.0713, 0.0820],\n",
      "        [0.0438, 0.0478, 0.0394, 0.6230, 0.1046, 0.0660, 0.0753],\n",
      "        [0.0756, 0.0845, 0.0515, 0.4776, 0.0940, 0.0887, 0.1281],\n",
      "        [0.0648, 0.1248, 0.0550, 0.3981, 0.0872, 0.1058, 0.1642],\n",
      "        [0.1040, 0.1548, 0.0999, 0.2648, 0.1111, 0.1158, 0.1498],\n",
      "        [0.0844, 0.1257, 0.0596, 0.3782, 0.0840, 0.0998, 0.1683],\n",
      "        [0.0260, 0.0358, 0.0263, 0.7050, 0.0912, 0.0472, 0.0686],\n",
      "        [0.0546, 0.1234, 0.0605, 0.3714, 0.0974, 0.1071, 0.1857],\n",
      "        [0.0427, 0.0465, 0.0360, 0.6234, 0.0998, 0.0718, 0.0798],\n",
      "        [0.0442, 0.0757, 0.0507, 0.5100, 0.1226, 0.0773, 0.1197],\n",
      "        [0.0587, 0.1320, 0.0566, 0.3773, 0.0858, 0.1048, 0.1847],\n",
      "        [0.0468, 0.0596, 0.0484, 0.5579, 0.1200, 0.0722, 0.0950],\n",
      "        [0.1018, 0.1428, 0.0757, 0.3091, 0.0993, 0.1201, 0.1513],\n",
      "        [0.0737, 0.1100, 0.0638, 0.4129, 0.1095, 0.0977, 0.1324],\n",
      "        [0.0302, 0.0453, 0.0200, 0.6974, 0.0601, 0.0559, 0.0911],\n",
      "        [0.1238, 0.1725, 0.1219, 0.2036, 0.1078, 0.1208, 0.1495],\n",
      "        [0.1119, 0.1671, 0.1271, 0.2005, 0.1138, 0.1177, 0.1619],\n",
      "        [0.0813, 0.1524, 0.0647, 0.3466, 0.0846, 0.1068, 0.1636],\n",
      "        [0.0404, 0.0873, 0.0400, 0.5182, 0.0859, 0.0915, 0.1367],\n",
      "        [0.0225, 0.0454, 0.0506, 0.5764, 0.1493, 0.0694, 0.0864],\n",
      "        [0.1016, 0.1537, 0.0939, 0.2691, 0.1038, 0.1144, 0.1634],\n",
      "        [0.0293, 0.0906, 0.0818, 0.3852, 0.1758, 0.0962, 0.1411],\n",
      "        [0.0749, 0.0905, 0.0585, 0.4587, 0.1073, 0.0890, 0.1212],\n",
      "        [0.0895, 0.1915, 0.1025, 0.2196, 0.0975, 0.1172, 0.1821],\n",
      "        [0.0669, 0.1427, 0.0820, 0.2997, 0.1092, 0.1091, 0.1904],\n",
      "        [0.0859, 0.1184, 0.0838, 0.3423, 0.1185, 0.1047, 0.1463],\n",
      "        [0.0884, 0.1399, 0.0851, 0.3132, 0.1099, 0.1072, 0.1562],\n",
      "        [0.0837, 0.1317, 0.0720, 0.3458, 0.0991, 0.1158, 0.1518],\n",
      "        [0.0907, 0.1252, 0.0885, 0.3283, 0.1200, 0.1023, 0.1450],\n",
      "        [0.0259, 0.0405, 0.0287, 0.6719, 0.0926, 0.0585, 0.0819],\n",
      "        [0.0271, 0.0378, 0.0177, 0.7349, 0.0638, 0.0450, 0.0735],\n",
      "        [0.0372, 0.0492, 0.0362, 0.6292, 0.1048, 0.0627, 0.0807],\n",
      "        [0.0933, 0.1463, 0.0813, 0.3138, 0.0965, 0.1151, 0.1537],\n",
      "        [0.0655, 0.1411, 0.1150, 0.2506, 0.1407, 0.1116, 0.1756],\n",
      "        [0.0710, 0.1575, 0.0981, 0.2594, 0.1220, 0.1084, 0.1837],\n",
      "        [0.0438, 0.0703, 0.0536, 0.5127, 0.1296, 0.0788, 0.1112],\n",
      "        [0.0843, 0.1293, 0.0610, 0.3783, 0.0857, 0.1118, 0.1496],\n",
      "        [0.0379, 0.0426, 0.0217, 0.6949, 0.0642, 0.0563, 0.0824],\n",
      "        [0.0051, 0.0085, 0.0054, 0.8962, 0.0380, 0.0166, 0.0302],\n",
      "        [0.0735, 0.0980, 0.0477, 0.4743, 0.0858, 0.0927, 0.1281],\n",
      "        [0.0469, 0.0535, 0.0315, 0.6258, 0.0824, 0.0663, 0.0936],\n",
      "        [0.1245, 0.1638, 0.1056, 0.2386, 0.1074, 0.1214, 0.1388],\n",
      "        [0.0379, 0.0644, 0.0355, 0.5816, 0.0959, 0.0682, 0.1165],\n",
      "        [0.0976, 0.1631, 0.0873, 0.2684, 0.0935, 0.1120, 0.1780],\n",
      "        [0.0405, 0.0436, 0.0220, 0.6860, 0.0655, 0.0556, 0.0869],\n",
      "        [0.0611, 0.0867, 0.0571, 0.4664, 0.1127, 0.0845, 0.1316],\n",
      "        [0.0398, 0.0440, 0.0342, 0.6444, 0.1001, 0.0628, 0.0747],\n",
      "        [0.0949, 0.1224, 0.0861, 0.3347, 0.1178, 0.1022, 0.1419],\n",
      "        [0.0896, 0.0761, 0.0569, 0.4956, 0.1066, 0.0832, 0.0920],\n",
      "        [0.0499, 0.0645, 0.0589, 0.5136, 0.1302, 0.0857, 0.0973],\n",
      "        [0.0483, 0.0924, 0.0542, 0.4623, 0.1111, 0.0910, 0.1407],\n",
      "        [0.1241, 0.1719, 0.1176, 0.2067, 0.1105, 0.1189, 0.1503],\n",
      "        [0.0734, 0.1413, 0.1015, 0.2771, 0.1268, 0.1123, 0.1676],\n",
      "        [0.0824, 0.1407, 0.0965, 0.2736, 0.1164, 0.1226, 0.1677],\n",
      "        [0.0984, 0.1851, 0.0913, 0.2408, 0.1059, 0.1249, 0.1535],\n",
      "        [0.0937, 0.1469, 0.0766, 0.3200, 0.1006, 0.1196, 0.1426],\n",
      "        [0.0478, 0.0407, 0.0259, 0.6832, 0.0736, 0.0589, 0.0698],\n",
      "        [0.0468, 0.0584, 0.0445, 0.5630, 0.1122, 0.0731, 0.1020],\n",
      "        [0.0586, 0.1017, 0.0408, 0.4874, 0.0763, 0.0961, 0.1391],\n",
      "        [0.0417, 0.0481, 0.0254, 0.6697, 0.0732, 0.0651, 0.0768],\n",
      "        [0.0829, 0.1192, 0.0669, 0.3831, 0.1019, 0.1047, 0.1414],\n",
      "        [0.0536, 0.1055, 0.0832, 0.3577, 0.1466, 0.1028, 0.1506],\n",
      "        [0.0570, 0.0830, 0.0400, 0.5192, 0.0797, 0.0877, 0.1334],\n",
      "        [0.0805, 0.1196, 0.0581, 0.4013, 0.0881, 0.1069, 0.1455],\n",
      "        [0.0703, 0.1406, 0.0799, 0.3174, 0.1111, 0.1035, 0.1772],\n",
      "        [0.0212, 0.0304, 0.0353, 0.6737, 0.1200, 0.0617, 0.0575],\n",
      "        [0.0963, 0.1436, 0.0939, 0.2946, 0.1127, 0.1068, 0.1522],\n",
      "        [0.0330, 0.0502, 0.0490, 0.5782, 0.1375, 0.0715, 0.0807],\n",
      "        [0.0672, 0.0724, 0.0528, 0.5074, 0.1082, 0.0838, 0.1082],\n",
      "        [0.0610, 0.1073, 0.0781, 0.3723, 0.1299, 0.1004, 0.1511],\n",
      "        [0.0376, 0.0483, 0.0269, 0.6622, 0.0790, 0.0549, 0.0912],\n",
      "        [0.0476, 0.0593, 0.0336, 0.5958, 0.0821, 0.0729, 0.1086],\n",
      "        [0.0321, 0.0467, 0.0365, 0.6280, 0.1074, 0.0643, 0.0850],\n",
      "        [0.0559, 0.0693, 0.0492, 0.5333, 0.1099, 0.0758, 0.1065],\n",
      "        [0.0483, 0.0553, 0.0460, 0.5658, 0.1206, 0.0739, 0.0902],\n",
      "        [0.1208, 0.1867, 0.1049, 0.2193, 0.1032, 0.1273, 0.1377],\n",
      "        [0.1294, 0.1384, 0.1027, 0.2675, 0.1096, 0.1140, 0.1385],\n",
      "        [0.0664, 0.1054, 0.0750, 0.3915, 0.1253, 0.0972, 0.1392],\n",
      "        [0.1267, 0.1583, 0.0917, 0.2741, 0.1030, 0.1214, 0.1248],\n",
      "        [0.1216, 0.1995, 0.1010, 0.2108, 0.1018, 0.1298, 0.1356],\n",
      "        [0.0310, 0.0337, 0.0208, 0.7313, 0.0698, 0.0481, 0.0652],\n",
      "        [0.0320, 0.0374, 0.0340, 0.6572, 0.1072, 0.0643, 0.0679],\n",
      "        [0.0746, 0.1542, 0.1085, 0.2453, 0.1290, 0.1050, 0.1834],\n",
      "        [0.0193, 0.0191, 0.0144, 0.8033, 0.0654, 0.0379, 0.0406],\n",
      "        [0.0369, 0.0286, 0.0201, 0.7485, 0.0661, 0.0473, 0.0526],\n",
      "        [0.0727, 0.1164, 0.0768, 0.3578, 0.1171, 0.1072, 0.1521],\n",
      "        [0.1247, 0.1823, 0.1314, 0.1851, 0.1112, 0.1168, 0.1485],\n",
      "        [0.0782, 0.1271, 0.0709, 0.3495, 0.0984, 0.1173, 0.1585],\n",
      "        [0.0860, 0.1393, 0.0721, 0.3478, 0.1018, 0.1142, 0.1387],\n",
      "        [0.0998, 0.1610, 0.1032, 0.2496, 0.1097, 0.1202, 0.1565],\n",
      "        [0.0507, 0.0745, 0.0536, 0.4910, 0.1192, 0.0861, 0.1250],\n",
      "        [0.0521, 0.0546, 0.0279, 0.6330, 0.0700, 0.0681, 0.0945],\n",
      "        [0.0777, 0.1239, 0.0534, 0.4101, 0.0804, 0.1081, 0.1465],\n",
      "        [0.0874, 0.1111, 0.0662, 0.3969, 0.1004, 0.1071, 0.1308],\n",
      "        [0.0409, 0.0922, 0.0314, 0.5311, 0.0667, 0.0848, 0.1529],\n",
      "        [0.0157, 0.0245, 0.0253, 0.7347, 0.1048, 0.0454, 0.0496],\n",
      "        [0.0981, 0.1676, 0.0931, 0.2599, 0.0988, 0.1083, 0.1742],\n",
      "        [0.0088, 0.0104, 0.0162, 0.8238, 0.0863, 0.0286, 0.0259],\n",
      "        [0.0640, 0.1048, 0.0772, 0.3826, 0.1329, 0.0975, 0.1411],\n",
      "        [0.0529, 0.0526, 0.0368, 0.6175, 0.0961, 0.0632, 0.0809],\n",
      "        [0.0833, 0.0859, 0.0700, 0.4421, 0.1224, 0.0903, 0.1061],\n",
      "        [0.0465, 0.0950, 0.0930, 0.3586, 0.1709, 0.1044, 0.1316],\n",
      "        [0.1072, 0.1502, 0.1079, 0.2485, 0.1122, 0.1208, 0.1531],\n",
      "        [0.0641, 0.0617, 0.0405, 0.5687, 0.0900, 0.0814, 0.0936],\n",
      "        [0.0690, 0.1808, 0.1096, 0.2046, 0.1136, 0.1107, 0.2117],\n",
      "        [0.0963, 0.1214, 0.0667, 0.3802, 0.0950, 0.1101, 0.1301],\n",
      "        [0.0726, 0.0944, 0.0603, 0.4306, 0.1053, 0.1005, 0.1363],\n",
      "        [0.0767, 0.0879, 0.0532, 0.4666, 0.0965, 0.0935, 0.1256],\n",
      "        [0.0836, 0.1319, 0.0615, 0.3809, 0.0853, 0.1002, 0.1566],\n",
      "        [0.0620, 0.0786, 0.0336, 0.5517, 0.0678, 0.0773, 0.1292]],\n",
      "       grad_fn=<SoftmaxBackward0>)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kevin\\OneDrive\\Documents\\GitHub\\data_augmentation\\notebooks\\03.bert_model - Copia.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/03.bert_model%20-%20Copia.ipynb#X14sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39m# Calculate accuracy for the current batch\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/03.bert_model%20-%20Copia.ipynb#X14sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39mprint\u001b[39m(torch\u001b[39m.\u001b[39msoftmax(logits, \u001b[39m1\u001b[39m))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/03.bert_model%20-%20Copia.ipynb#X14sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m _, predicted \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msoftmax(logits, \u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/03.bert_model%20-%20Copia.ipynb#X14sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/03.bert_model%20-%20Copia.ipynb#X14sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (predicted \u001b[39m==\u001b[39m labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Initialize variables to keep track of training statistics\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, tkn_type, att_mask, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "    \n",
    "        logits = model(inputs.squeeze(1), att_mask.squeeze(1), tkn_type.squeeze(1))\n",
    "        # print(outputs)\n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        # Backward pass and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy for the current batch\n",
    "        predicted = torch.softmax(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # Print training statistics for the epoch\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'batch_idx [{batch_idx + 1}/{len(train_loader)}], Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%', end='\\r')\n",
    "\n",
    "    # Print training statistics for the epoch\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch + 1}/10], Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final accuracy after training\n",
    "print('Final Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
