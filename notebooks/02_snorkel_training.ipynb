{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from transformers import BertModel\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomTextDataset(Dataset):\n",
    "    def __init__(self, text_data, labels, transform=None):\n",
    "        self.data = text_data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        if self.transform:\n",
    "            text = self.transform(text)\n",
    "        return text, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_og = pd.read_csv('../data/processed/train_og.csv')\n",
    "x_train_aug = pd.read_csv('../data/processed/train_augmented.csv')\n",
    "x_train_fin = pd.read_csv('../data/processed/train_final.csv')\n",
    "x_test = pd.read_csv('../data/processed/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titulo': ['introdução', 'à', 'teoria', 'da', 'literatura'],\n",
       " 'genero': ['literatura'],\n",
       " 'alt_title': ['introducao', 'teoria', 'literatura'],\n",
       " 'label': '6'}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Defining the feature processing\n",
    "\n",
    "TEXT = torchtext.data.Field(\n",
    "    tokenize='spacy', # default splits on whitespace\n",
    "    tokenizer_language='pt_core_news_sm', \n",
    "    # stop_words='pt_core_news_sm',\n",
    "    lower=True\n",
    ")\n",
    "\n",
    "### Defining the label processing\n",
    "LABEL = torchtext.data.LabelField(dtype=torch.int8)\n",
    "\n",
    "fields = [('titulo', TEXT), \n",
    "          ('genero', TEXT),  \n",
    "          ('alt_title', TEXT), \n",
    "          ('label', LABEL), \n",
    "          ('sf_historia', LABEL),\n",
    "          ('sf_administracao', LABEL), \n",
    "          ('sf_geografia', LABEL), \n",
    "          ('sf_biologia', LABEL), \n",
    "          ('sf_literatura', LABEL),\n",
    "          ('sf_artes', LABEL), \n",
    "          ('sf_matematica', LABEL)]\n",
    "\n",
    "x_train_og = torchtext.data.TabularDataset(\n",
    "    path='../data/processed/train_og.csv', format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "x_train_aug = torchtext.data.TabularDataset(\n",
    "    path='../data/processed/train_augmented.csv', format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "x_train_fin = torchtext.data.TabularDataset(\n",
    "    path='../data/processed/train_final.csv', format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "x_test = torchtext.data.TabularDataset(\n",
    "    path='../data/processed/test.csv', format='csv',\n",
    "    skip_header=True, fields=fields)\n",
    "\n",
    "vars(x_test.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_train_og.fields['titulo']\n",
    "del x_train_og.fields['genero']\n",
    "\n",
    "del x_train_aug.fields['titulo']\n",
    "del x_train_aug.fields['genero']\n",
    "\n",
    "del x_train_fin.fields['titulo']\n",
    "del x_train_fin.fields['genero']\n",
    "\n",
    "del x_test.fields['titulo']\n",
    "del x_test.fields['genero']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('historia', 128), ('brasil', 85), ('matematica', 64), ('geografia', 60), ('biologia', 49), ('geral', 44), ('administracao', 44), ('arte', 26), ('mundo', 23), ('espaco', 20)]\n"
     ]
    }
   ],
   "source": [
    "TEXT.build_vocab(x_train_og)\n",
    "\n",
    "print(TEXT.vocab.freqs.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader ,test_loader = \\\n",
    "torchtext.data.BucketIterator.splits(\n",
    "        (x_train_og, x_test),\n",
    "         batch_size=BATCH_SIZE,\n",
    "         sort_within_batch=False,\n",
    "         sort_key=lambda x: len(x.alt_title),\n",
    "         ) \n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.iterator.BucketIterator at 0x1cf99fa9050>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'titulo': ['biologia', 'dos', 'organismos'],\n",
       " 'genero': ['biologia'],\n",
       " 'alt_title': ['biologia', 'organismos'],\n",
       " 'label': '3'}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(train_loader.dataset.examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LabelField' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\kevin\\OneDrive\\Documents\\GitHub\\data_augmentation\\notebooks\\02_snorkel_training.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/02_snorkel_training.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mTrain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/02_snorkel_training.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/02_snorkel_training.ipynb#X25sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mText matrix size: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m.\u001b[39malt_title\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/kevin/OneDrive/Documents/GitHub/data_augmentation/notebooks/02_snorkel_training.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTarget vector size: \u001b[39m\u001b[39m{\u001b[39;00mbatch\u001b[39m.\u001b[39mlable\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\iterator.py:156\u001b[0m, in \u001b[0;36mIterator.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m             minibatch\u001b[39m.\u001b[39msort(key\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_key, reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m--> 156\u001b[0m     \u001b[39myield\u001b[39;00m Batch(minibatch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    157\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepeat:\n\u001b[0;32m    158\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\batch.py:34\u001b[0m, in \u001b[0;36mBatch.__init__\u001b[1;34m(self, data, dataset, device)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mif\u001b[39;00m field \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     batch \u001b[39m=\u001b[39m [\u001b[39mgetattr\u001b[39m(x, name) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m data]\n\u001b[1;32m---> 34\u001b[0m     \u001b[39msetattr\u001b[39m(\u001b[39mself\u001b[39m, name, field\u001b[39m.\u001b[39;49mprocess(batch, device\u001b[39m=\u001b[39;49mdevice))\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\field.py:237\u001b[0m, in \u001b[0;36mField.process\u001b[1;34m(self, batch, device)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\" Process a list of examples to create a torch.Tensor.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \n\u001b[0;32m    228\u001b[0m \u001b[39mPad, numericalize, and postprocess a batch and create a tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39m    and custom postprocessing Pipeline.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    236\u001b[0m padded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpad(batch)\n\u001b[1;32m--> 237\u001b[0m tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnumericalize(padded, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    238\u001b[0m \u001b[39mreturn\u001b[39;00m tensor\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\field.py:338\u001b[0m, in \u001b[0;36mField.numericalize\u001b[1;34m(self, arr, device)\u001b[0m\n\u001b[0;32m    336\u001b[0m     arr \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ex] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     arr \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstoi[x] \u001b[39mfor\u001b[39;49;00m x \u001b[39min\u001b[39;49;00m arr]\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing(arr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n",
      "File \u001b[1;32mc:\\Users\\kevin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchtext\\data\\field.py:338\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    336\u001b[0m     arr \u001b[39m=\u001b[39m [[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m ex] \u001b[39mfor\u001b[39;00m ex \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     arr \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39mstoi[x] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m arr]\n\u001b[0;32m    340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    341\u001b[0m     arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpostprocessing(arr, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocab)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'LabelField' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "print('Train')\n",
    "for batch in train_loader:\n",
    "    print(f'Text matrix size: {batch.alt_title.size()}')\n",
    "    print(f'Target vector size: {batch.lable.size()}')\n",
    "    break\n",
    "    \n",
    "print('\\nTest:')\n",
    "for batch in test_loader:\n",
    "    print(f'Text matrix size: {batch.alt_title.size()}')\n",
    "    print(f'Target vector size: {batch.lable.size()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input_tensor, hidden_tensor):\n",
    "        combined = torch.cat((input_tensor, hidden_tensor), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(GRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        out, hn = self.gru(x, h0.detach())\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, nhead, nhid, nlayers, dropout=0.5):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = nn.Sequential(\n",
    "            nn.Linear(ninp, nhid),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(nhid, nhead))\n",
    "        encoder_layers = TransformerEncoderLayer(nhead, nhid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.ninp = ninp\n",
    "        self.decoder = nn.Linear(ninp, ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "\n",
    "        src = self.encoder(src) * math.sqrt(self.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs[1]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
